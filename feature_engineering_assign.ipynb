{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q-1) What is a parameter?\n",
        "\n",
        "Ans) In feature engineering, a parameter is a value that defines or influences the behavior of a feature transformation or extraction process.\n",
        "\n",
        "-> User-defined or preset: For example, setting a threshold for binarizing a feature (e.g., values above 0.5 become 1, and below become 0).\n",
        "\n",
        "-> Learned from data: Sometimes, parameters are estimated during model training (like the weights in a regression model), though these are often considered model parameters rather than feature engineering parameters.\n",
        "\n",
        "-> parameters in feature engineering are crucial because they control how raw data is transformed into features that a model can use effectively.\n",
        "\n",
        "-> Adjusting these parameters can significantly affect the quality of the features and, subsequently, the model`s performance.\n",
        "\n",
        "-> Binning: Choosing the number of bins or the boundaries of bins when discretizing a continuous feature.\n",
        "\n",
        "-> Normalization/Scaling: Deciding which scaling method to use (min-max, z-score) and calculating the necessary scaling factors (e.g., mean and standard deviation)."
      ],
      "metadata": {
        "id": "7vbNPc2m3C6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-2) What is correlation? What does negative correlation mean?\n",
        "\n",
        "Ans) Correlation is a statistical measure that describes the relationship between two features (variables).\n",
        "\n",
        "->  It indicates how one feature changes in relation to another. Correlation values range from -1 to 1:\n",
        "\n",
        "* 1 → Perfect positive correlation (both increase together).\n",
        "\n",
        "* 0 → No correlation (no relationship between them).\n",
        "\n",
        "* -1 → Perfect negative correlation (one increases while the other decreases).\n",
        "\n",
        "-> In feature engineering, correlation helps in feature selection by identifying redundant or irrelevant features.\n",
        "\n",
        "-> A negative correlation means that as one feature increases, the other decreases. This is represented by a correlation value between -1 and 0.\n",
        "\n",
        "-> Strong negative correlation might indicate redundant features that can be removed.\n",
        "\n",
        "-> If two features are highly negatively correlated, keeping both might not add value to the model.\n"
      ],
      "metadata": {
        "id": "HZ6gRWvU4c3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-3) Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Ans) Machine Learning (ML) is a branch of artificial intelligence (AI) that enables computers to learn patterns from data and make predictions or decisions without being explicitly programmed.\n",
        "\n",
        "->  Instead of using predefined rules, ML models identify patterns and relationships within data to generalize and make informed predictions\n",
        "\n",
        "* Dataset\n",
        "\n",
        "-> A collection of data used for training and testing the model.\n",
        "Can be structured (tables, databases) or unstructured (images, text).\n",
        "Features (Input Variables)\n",
        "\n",
        "-> Independent variables used to train the model.\n",
        "Feature engineering improves model performance by selecting and transforming relevant features.\n",
        "\n",
        "* Model (Algorithm)\n",
        "\n",
        "-> A mathematical structure that learns from data.\n",
        "Examples: Linear Regression, Decision Trees, Neural Networks.\n",
        "\n",
        "-> Evaluates how well the model`s predictions match the actual values.\n",
        "Examples: Mean Squared Error (MSE) for regression.\n",
        "\n",
        "* Optimization Algorithm\n",
        "\n",
        "-> Adjusts model parameters to minimize errors.\n",
        "Example: Gradient Descent is commonly used in neural networks.\n",
        "\n",
        "* Training Process\n",
        "\n",
        "-> The phase where the model learns patterns from labeled data by adjusting its parameters.\n",
        "\n",
        "* Evaluation Metrics\n",
        "\n",
        "-> Used to assess model performance on unseen data.\n",
        "Examples: Accuracy, Precision-Recall, F1 Score.\n",
        "\n",
        "-> Used to check how well the model generalizes to new data.\n",
        "Prevents overfitting by ensuring the model isn`t memorizing training data.\n",
        "\n",
        "* Deployment & Monitoring\n",
        "\n",
        "-> Once trained, the model is deployed for real-world use.\n",
        "Continuous monitoring ensures the model remains accurate over time."
      ],
      "metadata": {
        "id": "uU8lMd9I44ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-4) How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Ans) The loss value is a numerical measure of how well a machine learning model's predictions match the actual target values.\n",
        "\n",
        "->  It helps determine whether the model is learning effectively or if it needs further improvements.\n",
        "\n",
        "-> A lower loss value means the model is making more accurate predictions.\n",
        "\n",
        "-> A higher loss value suggests the model is performing poorly.\n",
        "\n",
        "-> The model updates its parameters (weights) to minimize the loss during training.\n",
        "\n",
        "-> This process is done using optimization algorithms like Gradient Descent.\n",
        "\n",
        "-> Regression models:\n",
        "\n",
        "Mean Squared Error (MSE)\n",
        "Mean Absolute Error (MAE)\n",
        "\n",
        "-> Classification models:\n",
        "\n",
        "Cross-Entropy Loss (Log Loss)\n",
        "Hinge Loss (for SVMs)\n"
      ],
      "metadata": {
        "id": "RcuYQ0e-9iHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-5) What are continuous and categorical variables?\n",
        "\n",
        "Ans)  variables (features) are broadly classified into two types:\n",
        "\n",
        "-> A continuous variable can take any numerical value within a given range. These values are measurable and often have decimal points.\n",
        "\n",
        "Examples:\n",
        "\n",
        "-> Height (in cm) → 175.5 cm\n",
        "\n",
        "Infinite possible values in a range (e.g., 0.1, 0.2, 0.3… up to 100).\n",
        "\n",
        "-> A categorical variable represents distinct groups or categories, rather than numerical values.\n",
        "\n",
        "**Types of Categorical Variables:**\n",
        "\n",
        "* Nominal Variables (No Order):\n",
        "\n",
        "Gender → Male, Female, Other\n",
        "Blood Group → A, B, AB, O\n",
        "\n",
        "* Ordinal Variables (Has Order/Ranking):\n",
        "\n",
        "Education Level → High School, Bachelor`s, Master's, PhD\n",
        "Customer Satisfaction → Low, Medium, High\n",
        "\n",
        "->  Categorical variables require encoding since ML models work with numbers, not text.\n",
        "\n",
        "->  Understanding variable types helps in feature selection & transformation for better model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "i4t5us_j-HAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-6) How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Ans) Machine Learning models require numerical inputs, so categorical variables must be transformed into numerical values.\n",
        "\n",
        "-> The process of converting categorical data into a format that ML models can understand is called encoding.\n",
        "\n",
        "* One-Hot Encoding (OHE)\n",
        "\n",
        "->  Converts each category into a separate binary (0 or 1) column.\n",
        "\n",
        "->  Used for nominal (unordered) categories.\n",
        "\n",
        "* Label Encoding\n",
        "\n",
        "-> Assigns a unique integer to each category.\n",
        "\n",
        "->  Used for ordinal (ordered) categories.\n",
        "\n",
        "* Ordinal Encoding\n",
        "\n",
        "->  Similar to Label Encoding but only used when the categories have a meaningful order.\n",
        "\n",
        "*  Target Encoding (Mean Encoding)\n",
        "\n",
        "-> Replaces categories with the mean of the target variable.\n",
        "\n",
        "-> Used in supervised learning problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "j0yVtrrF-9yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-7) What do you mean by training and testing a dataset?\n",
        "\n",
        "Ans) In Machine Learning, a dataset is typically split into training and testing sets to evaluate a model's performance on unseen data.\n",
        "\n",
        "* Training Dataset\n",
        "\n",
        "->  The training dataset is used to train the machine learning model.\n",
        "\n",
        "-> The model learns patterns, relationships, and features from this data.\n",
        "\n",
        "-> It helps the model adjust its internal parameters (weights).\n",
        "\n",
        "* Testing Dataset\n",
        "\n",
        "->  The testing dataset is used to evaluate the model's performance.\n",
        "\n",
        "->  It contains new data that the model has never seen before.\n",
        "\n",
        "->  Helps check if the model generalizes well or if it is overfitting.\n",
        "\n",
        " A common split ratio:\n",
        "\n",
        "* 80% Training Data\n",
        "* 20% Testing Data\n",
        "* Other ratios: 70-30, 90-10, 60-40 (depends on dataset size)."
      ],
      "metadata": {
        "id": "_eme8gwF_pW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-8) What is sklearn.preprocessing?\n",
        "\n",
        "Ans) sklearn.preprocessing is a module in Scikit-Learn that provides various data preprocessing techniques to transform raw data into a format suitable for machine learning models.\n",
        "\n",
        "->  It includes methods for scaling, normalization, encoding, and imputation.\n",
        "\n",
        "**Functions in sklearn.preprocessing:**\n",
        "\n",
        "* Standardization (Z-score Scaling)\n",
        "\n",
        "->  Ensures data has mean = 0 and standard deviation = 1\n",
        "\n",
        "->  Helps models like Logistic Regression and SVM perform better.\n",
        "\n",
        "*  Min-Max Scaling (Normalization)\n",
        "\n",
        "->  Scales data between a fixed range (default: 0 to 1)\n",
        "\n",
        "->  Useful for models that rely on distance (e.g., KNN, Neural Networks)\n",
        "\n",
        "*  Label Encoding\n",
        "\n",
        "->  Converts categorical labels into numerical values.\n",
        "\n",
        "->  Used for ordinal categorical data.\n",
        "\n",
        "-> Ensures numerical stability (important for algorithms like Gradient Descent)\n",
        "\n",
        "->  Improves model accuracy (badly scaled data can lead to poor performance)\n",
        "\n",
        "-> Handles categorical data automatically."
      ],
      "metadata": {
        "id": "zTIDamZdA0ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-9) What is a Test set?\n",
        "\n",
        "Ans) A test set is a subset of a dataset that is used to evaluate the performance of a trained machine learning model.\n",
        "\n",
        "-> It contains new, unseen data that was not used during training.\n",
        "\n",
        "->  Measures Model Performance → Helps check how well the model generalizes to new data.\n",
        "\n",
        "->  Prevents Overfitting : Ensures the model is not just memorizing training data.\n",
        "\n",
        "-> Used for Final Evaluation : After training and validation, the test set provides the final accuracy, precision, recall, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y7paHb30BtV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([10, 20, 30, 40, 50])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Data:\", X_train)\n",
        "print(\"Test Data:\", X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uepCkUHCB-6I",
        "outputId": "6e709485-906b-41b8-a3c7-9bf7616cdbb7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data: [[5]\n",
            " [3]\n",
            " [1]\n",
            " [4]]\n",
            "Test Data: [[2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-10) How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        "Ans) In Machine Learning, we split the dataset into a training set (for model learning) and a test set (for evaluation).\n",
        "\n",
        "-> This is done using the train_test_split() function from sklearn.model_selection.\n",
        "\n",
        "*  Splitting Data Using train_test_split()\n",
        "\n",
        "->  Default split: 80% training, 20% testing (can be adjusted).\n",
        "\n",
        "-> Random state ensures reproducibility of the split.\n",
        "\n",
        "* Customizing the Train-Test Split\n",
        "\n",
        "->  Change test size: Use test_size=0.3 for 70-30 split, test_size=0.1 for 90-10 split.\n",
        "\n",
        "->  Stratify split: If the dataset is imbalanced, use stratify=y.\n",
        "\n",
        "->  Shuffle data: Enabled by default (shuffle=True), but can be disabled (shuffle=False).\n",
        "\n",
        "* Splitting Data into Train, Validation, and Test Sets\n",
        "\n",
        "->  Sometimes, we also use a validation set to tune hyperparameters.\n",
        "\n",
        "->  Common split: 70% training, 15% validation, 15% testing.\n",
        "\n",
        "*  Approaching machine-learning pattern:\n",
        "\n",
        "->  Understand the problem before jumping into coding.\n",
        "\n",
        "->  Preprocess data to clean and structure it for models.\n",
        "\n",
        "->  Train and test models using appropriate ML techniques.\n",
        "\n",
        "->  Evaluate and optimize models for better accuracy.\n",
        "\n",
        "->  Deploy the model and monitor its real-world performance."
      ],
      "metadata": {
        "id": "s1tpvXM2CF3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-11) Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Ans) Performing EDA is a crucial step before fitting a machine learning model for several reasons.\n",
        "\n",
        "-> It helps us understand the data better and ensure we are well-prepared to build an effective and reliable model.\n",
        "\n",
        "->  Identify feature types (numerical, categorical, datetime) and their distributions.\n",
        "\n",
        "->  Helps in selecting the appropriate model and preprocessing techniques (e.g., scaling, encoding).\n",
        "\n",
        "-> Handle missing data appropriately before fitting the model.\n",
        "\n",
        "->  Missing data can lead to biased or inaccurate model results if not treated.\n",
        "\n",
        "->  Outliers can have a significant impact on certain models (e.g., Linear Regression, KNN).\n",
        "\n",
        "->  Detect and handle outliers to prevent misleading results.\n",
        "\n",
        "->  Identify correlations between features to better understand dependencies.\n",
        "\n",
        "->  Highly correlated features can cause multicollinearity in models like Linear Regression.\n",
        "\n",
        "->  Feature engineering opportunities arise from understanding feature relationships.\n",
        "\n"
      ],
      "metadata": {
        "id": "jq7YKrQlDbeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-12) What is correlation?\n",
        "\n",
        "Ans) Correlation refers to a statistical relationship or association between two or more variables.\n",
        "\n",
        "-> It measures the extent to which one variable changes when another variable changes.\n",
        "\n",
        "-> The correlation value lies between -1 and +1, and it indicates both the strength and direction of the relationship.\n",
        "\n",
        "**Types of Correlation**\n",
        "\n",
        "* Positive Correlation (+1)\n",
        "\n",
        "-> Both variables increase or decrease together.\n",
        "\n",
        "Example: As the number of study hours increases, the exam score tends to increase.\n",
        "\n",
        "* Negative Correlation (-1)\n",
        "\n",
        "-> One variable increases while the other decreases.\n",
        "\n",
        "Example: As the amount of sleep decreases, fatigue increases.\n",
        "\n",
        "* No Correlation (0)\n",
        "\n",
        "-> No linear relationship between the variables.\n",
        "\n",
        "Example: Shoe size and intelligence likely have no correlation.\n",
        "\n",
        "-> Correlation coefficient quantifies this relationship, with values between -1 and +1.\n",
        "\n",
        "-> Pearson's correlation is used for linear relationships, and Spearman's rank correlation is used for monotonic relationships.\n"
      ],
      "metadata": {
        "id": "Pz8jLTpEEA4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-13) What does negative correlation mean?\n",
        "\n",
        "Ans) Negative correlation refers to a statistical relationship between two variables where as one variable increases, the other decreases.\n",
        "\n",
        "->  In simpler terms, they move in opposite directions. If one variable goes up, the other tends to go down, and vice versa.\n",
        "\n",
        "**Key Characteristics of Negative Correlation:**\n",
        "\n",
        "* Inverse Relationship:\n",
        "\n",
        "-> When one variable increases, the other decreases.\n",
        "\n",
        "Example: Temperature and heating costs — As temperature increases, the need for heating decreases.\n",
        "\n",
        "* Correlation Coefficient:\n",
        "\n",
        "-> The correlation coefficient (r) for a negative correlation is less than 0 (but greater than -1).\n",
        "\n",
        "-> A perfect negative correlation has an r value of -1.\n",
        "\n",
        "-> A strong negative correlation has an r value closer to -1 (e.g., -0.8).\n",
        "\n",
        "-> A weak negative correlation has an r value closer to 0 but still negative (e.g., -0.2).\n",
        "\n",
        "-> Negative correlation means one variable increases while the other decreases.\n",
        "\n",
        "-> Correlation coefficient for negative correlation is between 0 and -1.\n",
        "\n",
        "-> A strong negative correlation has a coefficient close to -1, while a weak negative correlation is closer to 0.\n"
      ],
      "metadata": {
        "id": "vOFEeZ-2E4xD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-14) How can you find correlation between variables in Python?\n",
        "\n",
        "Ans) Pandas provides a built-in function corr() to calculate the correlation between numerical variables in a DataFrame.\n",
        "\n"
      ],
      "metadata": {
        "id": "f5uPDO6jFayl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'age': [25, 30, 35, 40, 45],\n",
        "    'height': [150, 160, 170, 180, 190],\n",
        "    'weight': [55, 60, 65, 70, 75]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4__MkF3sFrXy",
        "outputId": "46160606-8130-4e1a-a08d-732459f85d58"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        age  height  weight\n",
            "age     1.0     1.0     1.0\n",
            "height  1.0     1.0     1.0\n",
            "weight  1.0     1.0     1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-15) What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Ans) Causation refers to a cause-and-effect relationship between two variables, where one variable directly influences or causes a change in the other.\n",
        "\n",
        "->  In other words, causation indicates that a change in one variable directly results in a change in another.\n",
        "\n",
        "* Cause: The factor or action that brings about the change.\n",
        "\n",
        "* Effect: The change that occurs as a result of the cause.\n",
        "\n",
        "**Difference Between Correlation and Causation:**\n",
        "\n",
        "* Correlation:\n",
        "\n",
        "->  Correlation is a statistical relationship between two variables. It tells you how two variables move relative to each other, but it does not imply that one variable causes the other to change.\n",
        "\n",
        "-> Does not imply a cause-effect relationship.\n",
        "\n",
        "* Causation:\n",
        "\n",
        "-> Causation indicates that one variable directly causes a change in another. If X causes Y, it means changes in X lead to changes in Y. Causation implies a cause-effect relationship.\n",
        "\n",
        "-> Implies cause-and-effect relationship.\n",
        "\n",
        "\n",
        "-> Correlation can be misleading, making us think that two variables are directly related when they may not be. For example, correlation does not mean causation.\n",
        "\n",
        "-> Causation is harder to prove but much more valuable because it shows a true cause-and-effect relationship that we can use to predict or prevent outcomes."
      ],
      "metadata": {
        "id": "n_HufPbAF0Z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-16) What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "Ans) In machine learning, an optimizer is an algorithm or method used to minimize (or maximize) a loss function by adjusting the parameters of a model (e.g., weights in a neural network).\n",
        "\n",
        "->  The goal of an optimizer is to find the best parameters that reduce the error (or loss) during training, so the model can make accurate predictions on new data.\n",
        "\n",
        "-> Optimizers play a crucial role in training machine learning models, especially in gradient-based optimization, where they adjust model parameters iteratively based on the gradient of the loss function.\n",
        "\n",
        "* Gradient Descent\n",
        "\n",
        "-> Gradient Descent (GD) is the most basic and widely used optimization algorithm.\n",
        "\n",
        "->  It works by computing the gradient (or derivative) of the loss function with respect to the model parameters and then updating the parameters in the direction that minimizes the loss.\n",
        "\n",
        "* Stochastic Gradient Descent (SGD)\n",
        "\n",
        "-> Stochastic Gradient Descent (SGD) is a variant of Gradient Descent, where instead of using the entire dataset to compute the gradient, a single data point (or a small batch) is used at each iteration.\n",
        "\n",
        "* Adam (Adaptive Moment Estimation)\n",
        "\n",
        "-> Adam combines the benefits of Momentum and RMSProp (another adaptive learning rate optimizer).\n",
        "\n",
        "-> It computes adaptive learning rates for each parameter by considering both the first moment (mean) and the second moment (uncentered variance) of the gradients."
      ],
      "metadata": {
        "id": "bQVeAA9aGpdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-17) What is sklearn.linear_model ?\n",
        "\n",
        "Ans) In scikit-learn, sklearn.linear_model is a module that contains various linear models for regression and classification tasks.\n",
        "\n",
        "->  These models are based on the concept of linear relationships, meaning they aim to predict the target variable as a linear combination of the input features.\n",
        "\n",
        "-> This module includes commonly used linear algorithms such as Linear Regression, Logistic Regression, Ridge Regression, Lasso, and others.\n",
        "\n",
        "**Classes in sklearn.linear_model**\n",
        "\n",
        "* Linear Regression is suitable for most regression problems unless you have a reason to regularize the model.\n",
        "\n",
        "* Logistic Regression is the go-to for binary classification tasks.\n",
        "\n",
        "* Ridge is useful when you have multicollinearity or many features that are highly correlated.\n",
        "\n",
        "* Lasso is ideal when you need to perform feature selection.\n",
        "\n",
        "* ElasticNet is useful when your data is large and you want to benefit from both L1 and L2 regularization."
      ],
      "metadata": {
        "id": "IHNYxss5HOwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-18) What does model.fit() do? What arguments must be given?\n",
        "\n",
        "Ans) The fit() method is used to train a machine learning model. It adjusts the model's internal parameters (like weights in a linear model) based on the provided training data.\n",
        "\n",
        "->  In other words, it learns the patterns from the input data and uses this information to make predictions or classifications on new data.\n",
        "\n",
        "-> Learns from the training data (X_train as features and y_train as target labels).\n",
        "\n",
        "-> Adjusts its internal parameters (like coefficients, weights, or other model-specific parameters) to minimize the error or loss function.\n",
        "\n",
        "**Arguments for model.fit()**\n",
        "\n",
        "* X (features): A 2D array or matrix representing the input features of your dataset.\n",
        "\n",
        "-> This is the data that will be used to train the model.\n",
        "\n",
        "-> Shape: (n_samples, n_features) where n_samples is the number of data points (rows), and n_features is the number of features (columns).\n",
        "\n",
        "* y (target): A 1D array or vector representing the target values (labels) corresponding to the input data (X).\n",
        "\n",
        "-> This is the output or the label that the model tries to predict.\n",
        "\n",
        "-> Shape: (n_samples,), where n_samples is the number of data points."
      ],
      "metadata": {
        "id": "qoek0vuZH5FS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-19) What does model.predict() do? What arguments must be given?\n",
        "\n",
        "Ans) The predict() method is used to make predictions based on the trained model.\n",
        "\n",
        "->  After the model has been trained using the fit() method, the predict() method is called to generate predictions (outputs) for new, unseen data.\n",
        "\n",
        "-> It takes the input features as an argument and returns the predicted output.\n",
        "\n",
        "-> The model uses the parameters it learned during training (such as weights and biases) to compute the predictions for the new data.\n",
        "\n",
        "**Arguments for model.predict()**\n",
        "\n",
        "-> The predict() method generally requires the following argument:\n",
        "\n",
        "* X (features): A 2D array (or a similar structure) representing the input data for which you want to generate predictions.\n",
        "\n",
        "-> This input data should have the same number of features as the training data (i.e., X_train).\n",
        "\n",
        "-> Shape: (n_samples, n_features) where n_samples is the number of data points you want predictions for, and n_features is the number of features that the model was trained on."
      ],
      "metadata": {
        "id": "44L-AZwzIlOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-20) What are continuous and categorical variables?\n",
        "\n",
        "Ans) continuous and categorical. These classifications help determine the types of models, analysis techniques, and preprocessing steps to apply to the data.\n",
        "\n",
        "* Continuous Variables\n",
        "\n",
        "-> Continuous variables are variables that can take any value within a given range or scale.\n",
        "\n",
        "->  They are numerical and often represent quantities or measurements. Since these values are not restricted to specific, distinct values, they can take on an infinite number of possibilities within the range.\n",
        "\n",
        "-> Can take any value (including decimals) within a range.\n",
        "\n",
        "-> Represent measurements, counts, or magnitudes.\n",
        "\n",
        "-> Can be manipulated mathematically (addition, subtraction, multiplication, etc.).\n",
        "\n",
        "* Categorical Variables\n",
        "\n",
        "-> Categorical variables are variables that represent categories or groups.\n",
        "\n",
        "->  They take on a limited, fixed number of values and are typically used to describe qualities or characteristics rather than quantities.\n",
        "\n",
        "-> Categorical variables can be either nominal (no specific order) or ordinal (have a specific order).\n",
        "\n",
        "* Nominal:\n",
        "\n",
        "-> No intrinsic order or ranking between the categories.\n",
        "\n",
        "Examples: Gender,Color (red, blue, green)\n",
        "\n",
        "* Ordinal:\n",
        "\n",
        "-> Categories have a specific order or ranking.\n",
        "\n",
        "Examples: Education level (High School < Bachelor's < Master's),\n",
        "\n",
        "-> Limited, distinct categories (often strings or labels).\n",
        "\n",
        "-> No mathematical operations can be performed.\n",
        "\n",
        "-> Used to represent qualitative data."
      ],
      "metadata": {
        "id": "J0u55tUYJAYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-21) What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Ans) Feature scaling is the process of normalizing or standardizing the range of independent variables (features) in your dataset.\n",
        "\n",
        "-> This is important because the scale of the features can significantly affect the performance of certain machine learning models.\n",
        "\n",
        "->  In feature scaling, you ensure that all features are on a similar scale, so that no one feature dominates the learning process due to its larger values.\n",
        "\n",
        "-> Improves Model Performance: Many machine learning algorithms, especially those that rely on distance or gradients, are sensitive to the scale of features.\n",
        "\n",
        "-> If one feature has a much larger scale than others, it could disproportionately influence the model's performance.\n",
        "\n",
        "-> Ensures Fair Contribution: When features have different scales, models might give more importance to features with larger ranges (such as income or age in raw form).\n",
        "\n",
        "->  Feature scaling ensures that all features contribute equally to the model.\n",
        "\n",
        "-> Scaling is especially important for algorithms like KNN, SVM, and those using gradient-based optimization.\n",
        "\n",
        "-> Tree-based algorithms are generally not sensitive to feature scaling."
      ],
      "metadata": {
        "id": "l9r9ao1IJxs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-22) How do we perform scaling in Python?\n",
        "\n",
        "Ans) In Python, we can perform feature scaling using the scikit-learn library, which provides a set of utilities for scaling and normalizing features.\n",
        "\n",
        "->  Standardization, Min-Max Scaling, and Robust Scaling\n",
        "\n",
        "-> Use StandardScaler() for Z-score standardization.\n",
        "\n",
        "-> Use MinMaxScaler() for scaling to a specified range (e.g., [0, 1]).\n",
        "\n",
        "-> Use RobustScaler() when your data contains outliers and you want to use the median and IQR for scaling."
      ],
      "metadata": {
        "id": "fnLUc0rsKJt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[22, 40000],\n",
        "                 [35, 80000],\n",
        "                 [60, 120000],\n",
        "                 [45, 75000]])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(\"Standardized Data:\\n\", scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV-EE3qDKl66",
        "outputId": "7ee3be1f-a16d-4196-9107-e1e7aa4996fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized Data:\n",
            " [[-1.3307975  -1.36602321]\n",
            " [-0.3956425   0.04406526]\n",
            " [ 1.4027325   1.45415374]\n",
            " [ 0.3237075  -0.13219579]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-23) What is sklearn.preprocessing?\n",
        "\n",
        "Ans) sklearn.preprocessing is a module in scikit-learn (a Python library for machine learning) that provides various functions and classes to prepare and scale your data before feeding it into a machine learning model.\n",
        "\n",
        "->  Preprocessing refers to the operations you apply to your data to make it more suitable for learning algorithms, such as scaling, normalizing, encoding categorical variables, and more.\n",
        "\n",
        "* Feature Scaling: Normalizing or standardizing features to ensure that they are on a similar scale.\n",
        "\n",
        "* Encoding Categorical Variables: Converting categorical variables into numerical format.\n",
        "\n",
        "* Handling Missing Data: Filling missing values or removing data with missing values.\n",
        "\n",
        "* Transforming Data: Applying mathematical transformations like logarithmic scaling."
      ],
      "metadata": {
        "id": "S6VoIfGpKsLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-24) How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "Ans) To split data for model fitting (training and testing) in Python, we generally use train_test_split() from sklearn.model_selection.\n",
        "\n",
        "-> This function randomly splits a dataset into two subsets: one for training the model and another for testing the model's performance.\n",
        "\n",
        "-> This helps ensure that the model is evaluated on data it has not seen during training.\n",
        "\n",
        "**Steps for splitting data:**\n",
        "\n",
        "* Import Required Libraries: You need to import train_test_split from sklearn.model_selection.\n",
        "\n",
        "* Prepare Your Data: You should have your features (X) and labels/target (y) separated into variables.\n",
        "\n",
        "* Split the Data: Use the train_test_split() function to randomly split the data into training and testing sets.\n",
        "\n",
        "**syntax:**"
      ],
      "metadata": {
        "id": "M4bCCP-yLJqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "DXlBVoAZLj_6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> we should Use train_test_split() to divide your data into training and testing subsets.\n",
        "\n",
        "-> we can control the split ratio using the test_size parameter (e.g., 0.2 for an 80-20 split).\n",
        "\n",
        "-> The random_state ensures reproducibility.\n",
        "\n",
        "we should Use stratify for class imbalance in classification problems."
      ],
      "metadata": {
        "id": "hjKVBm1oLp16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-25) Explain data encoding?\n",
        "\n",
        "Ans) Data encoding is the process of converting categorical variables (variables with labels or categories) into a numerical format so that machine learning algorithms can work with them.\n",
        "\n",
        "->  Machine learning models typically expect numerical input, and encoding helps in converting these categorical variables into numerical values that can be fed into the model.\n",
        "\n",
        "-> There are different types of encoding techniques, each with its use case.\n",
        "\n",
        "*  Label Encoding (Integer Encoding)\n",
        "\n",
        "-> Label Encoding converts each category into a unique integer value. Each label in a categorical column is assigned an integer starting from 0.\n",
        "\n",
        "*  One-Hot Encoding\n",
        "\n",
        "-> One-Hot Encoding creates new binary columns for each unique category.\n",
        "\n",
        "-> Each column represents one category, and it will have a 1 if the instance belongs to that category and a 0 otherwise.\n",
        "\n",
        "*  Ordinal Encoding\n",
        "\n",
        "-> Ordinal Encoding is similar to Label Encoding but is specifically used when the categories have an ordinal relationship (e.g., small, medium, large).\n",
        "\n",
        "-> The order of the categories is important in this encoding.\n"
      ],
      "metadata": {
        "id": "V8WwICwaL3eL"
      }
    }
  ]
}